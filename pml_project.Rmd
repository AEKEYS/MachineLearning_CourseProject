---
title: "Better Weight Lifting Through Machine Learning"
author: "AEKEYS"
date: "February 21, 2015"
output: html_document
---

***Synopsis:*** This paper describes the process for building a classification model that predicts the quality of Unilateral Dumbbell Biceps Curls based on an exerciser's motion sensor output. It was written for the Johns Hopkins University's ["Practical Machine Learning"](https://www.coursera.org/course/predmachlearn) on Coursera. Data were obtained from Velloso, et al's study, ["Qualitative Activity Recognition of Weight Lifting Exercises"](http://groupware.les.inf.puc-rio.br/public/papers/2013.Velloso.QAR-WLE.pdf).

###Objective
The model's objective is to correctly classify an exerciser's motion into one of five classes based on the output of 12 motion sensors (accelerometer, gyroscope, and
magnetometer readings in the glove, arm, belt, and dumbbell). In this way, the model helps the exerciser determine *how well* he or she is performing a Dumbbell Bicep Curl.

The five classifications are:

*   (Class A) Exactly according to the specification,  
*   (Class B) Throwing the elbows to the front,  
*   (Class C) Lifting the dumbbell only halfway,  
*   (Class D) Lowering the dumbbell only halfway,  
*   (Class E) Throwing the hips to the front.

```{r echo=FALSE, results='hide', message=FALSE}
library(caret)
TrainingUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
TestUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

getData <- function(file, url) {
    # Gets file from web
    found <- F
    if (file %in% dir()) {
        found <- T
    }
    if (!found) {
        download.file(url, file, method = "curl")
    }
}
getData("pml-testing.csv", TestingUrl)
```

###Step 1: Load the data

The data are loaded from CSV from a URL provided by Coursera. Missing, blank, or undefined data are coded as NA.

```{r cache=TRUE}
getData("pml-training.csv", TrainingUrl)
WLE <- read.csv("pml-training.csv",
                header=TRUE,
                stringsAsFactors=FALSE,
                na.strings = c(""," ", "NA", "#DIV/0!"))
```

###Step 2: Clean the data

Data not relevant to the objective are identified and either reformatted or removed.

```{r echo=FALSE,results='hide'}
fishyColumns <- names(WLE)[which(sapply(WLE,class)=="logical")] #something weird here

for (i in 1:length(fishyColumns)){
    print(summary(WLE[,fishyColumns[i]])) #all are NAs
}

colToRemove <- which(names(WLE) %in% fishyColumns)

WLE <- WLE[,-c(colToRemove)] #remove features with all NAs
```
*  Upon inspection, `r length(fishyColumns)` sensors ("variables") output no information whatsoever. These are removed.  
*  Identifier variables in the data (e.g. observation labels, timing windows) will not be available in real-world applications, so they are removed. Timestamps are also removed, since data are already ordered sequentially (time structure is already "built in" to the data).
*  Sensors that did not record activity during certain time periods are recoded from NA to 0. Variables that recorded near zero variablilty over the entire period are discarded, since they represent no information about the activity being performed.

```{r echo=FALSE, results='hide'}
WLE <- transform(WLE, classe = factor(classe)) #outcome label
WLE <- transform(WLE, user_name = factor(user_name)) #subject name
WLE <- WLE[,-c(1,3:7)] #identifier variables that won't be available in real-world application, data is already ordered sequentially
WLE[,-c(1,148)] <- lapply(WLE[,-c(1, 148)],as.numeric) #make all others numeric
```

```{r cache=TRUE}
WLE[is.na(WLE)] <- 0 #replace NAs with 0 (the device does not measure any activity)
nzv <- nearZeroVar(WLE, saveMetrics = TRUE)
WLE <- WLE[,-c(which(names(WLE) %in% rownames(nzv)[nzv$nzv==TRUE]))] #remove nzv columns
```

###Step 3: Create training, validation, and test sets

* A ***training*** set is created from ~50% of the data for use in model training.
* A ***validation*** set is created from ~20% for use in model tuning and selection.
* A ***testing*** set (~30% of the data) is held in reserve as an unbiased estimator for out of sample error.
```{r echo=FALSE, results='hide', cache=TRUE}
set.seed(3223)
inTrain <- createDataPartition(y = WLE$classe,
                               p = .7,
                               list = FALSE)

training <- WLE[inTrain,] # use this for training and cross validation
independentTest <- WLE[-inTrain,] # reserve for independent test to estimate out of sample error

inTrain <- createDataPartition(y=training$classe,
                               p = .7,
                               list = FALSE)

training <- training[inTrain,] #use soley for training
validation <- training[-inTrain,] #use for cross validation, model tuning
```

###Step 4: Explore data

Read in data from one subject to get a better sense of how data are organized and how well the training sample represents all possible classifications. The chart below depicts four different sensor measures for user Carlitos. 

 *   Carlitos' activity in the training set is appears to be about for ~1.5 minutes of activity (1531 readings), meaning that each observation is .000964 of a minute or ~.05 seconds (50 would be about 2.5 seconds).  
 *   Carlitos performed the exercise in five different ways (A, B, C, D, E) sequentially.

```{r echo=FALSE}
carlitos <- subset(training, user_name=="carlitos")
par(mfrow=c(2,2))
plot(carlitos[,c("roll_belt")], col=carlitos$classe, ylab="roll_belt")
plot(carlitos[,c("pitch_forearm")], col=carlitos$classe, ylab="pitch_forearm")
plot(carlitos[,c("magnet_dumbbell_y")], col=carlitos$classe, ylab="magnet_dumbbell_y")
plot(carlitos[,c("roll_forearm")], col=carlitos$classe, ylab="roll_forearm")
legend("bottomright", legend=unique(carlitos$classe), col=unique(carlitos$classe), pch=1)
mtext("Carlitos' Activity", side = 3, line = -3, outer = TRUE)
par(mfrow=c(1,1))
```

###Step 5: Feature selection

To select the motion sensor readings (the variables or "features") that will be used to train the model, I begin by running a random forest model to let the data help select important features. Here, "importance" is defined as how well a particular variable partitions the data into the defined classes. The measure used is a mean decrease in Gini (where Gini is defined as "inequity" or "node impurity" in the tree classification).  

```{r cache=TRUE, message=FALSE}
# Random Forest Approach --> let the data help us select the features to use
set.seed(6432)
require(randomForest)
modFitRF <- randomForest(classe ~ . , 
                         strata = user_name,
                         data=training, importance=TRUE,
                        proximity=TRUE)
importantVar <- round(importance(modFitRF),2)
importantVar <- importantVar[order(importantVar[,7], decreasing = TRUE),]
###RF model
predRF <- predict(modFitRF, validation)
RFConfusionMat <- confusionMatrix(predRF, validation$classe)
```

I then iteratively train decision tree models using the most important variables from my ordered list. At each iteration I add an additional variable to the model and estimate its accuracy by calculating how well it does on a validation set. In this way, I am able to tune the model and determine the optimal number of variables to include. Importantly, I still maintain a separate test set that will be used as an unbiased estimator of the model's out-of-sample error.

```{r echo=FALSE,results='hide',cache=TRUE}
scores <- as.numeric(rep(0,times=20))

for (i in 1:length(scores)){
    
    fmla <- as.formula(paste("classe ~ ", paste(rownames(importantVar)[1:i], collapse= "+")))
    
    modFitTree <- train(fmla, 
                        method = "rpart",
                        data=training)
    pTree <- predict(modFitTree, validation)
    confMat <- confusionMatrix(pTree, validation$classe)
    scores[i] <- confMat$overall[[1]]
}
```

  *  The five variables with the highest mean decrease in Gini appear to be the optimal model for a single decision tree classification. Including additional variables past this points represents no gain to accuracy on the cross validation set.
  
```{r echo=FALSE}
plot(1:length(scores),scores, type="l",lwd=2, col="blue", 
     main="Tree Cross Validation Performance by No. Leafs",
     xlab="No. of included variables (ordered by MeanDecreaseGini)",
     ylab="Overall Accuracy")
```

  *  However, the model still performs poorly as indicated by a low accuracy rate and numerous misclassifications.  The confusion matrix and decision tree for the five-variable model would look as follows: 
  
```{r echo=FALSE, results='hide'}
fmla <- as.formula(paste("classe ~ ", paste(rownames(importantVar)[1:5], collapse= "+")))

modFitTree <- train(fmla, 
                    method = "rpart",
                    data=training)
pTree <- predict(modFitTree, validation)
confMat <- confusionMatrix(pTree, validation$classe)
```

```{r echo=FALSE,message=FALSE}
library(rattle)
confMat$table
fancyRpartPlot(modFitTree$finalModel) #best model for simple tree
```

###Step 6: Model selection

In total, I calculated accuracy scores on a validation set for four different types of models: the "five-variable" simple decision tree described above, a linear discriminant (LDA), a naive Bayes (NB) models, and a Random Forest model. I selected the Random Forest model, because it outperformed all the others on the validation set.

```{r echo=FALSE,results='hide',message=FALSE,warning=FALSE,include=FALSE,cache=TRUE}
###LDS model
modFitLDA <- train(classe ~ . -user_name,
                   data = training,
                   method = "lda")
pLDA <- predict(modFitLDA, validation)
LDAConfusionMat <- confusionMatrix(pLDA, validation$classe)

###NB model
modFitNB <- train(classe ~ roll_belt + magnet_dumbbell_z + pitch_forearm,
                  data = training,
                  method = "nb")
pNB <- predict(modFitNB, validation)
NBConfusionMat <- confusionMatrix(pNB, validation$classe) #appears to suffer from underrun
```

 *  Five-variable tree accuracy: `r confMat$overall[1]`
 *  LDA accuracy: `r LDAConfusionMat$overall[1]`
 *  NB accuracy: `r NBConfusionMat$overall[1]`
 *  ***Random Forest accuracy: `r RFConfusionMat$overall[1]`***
 
###Final Step: Checking accuracy on independent test set

Finally, I check the accuracy of the Random Forest model on an independent test set to estimate the out-of-sample error. These data were not used at any stage of the training or model tuning process and should represent a fair (unbiased) approximation for how well the model will perform on unseen data. The Random Forest model performs exceedingly well.

```{r message=FALSE}
require(caret); require(randomForest)
finalTest <- predict(modFitRF, independentTest)
confusionMatrix(finalTest, independentTest$classe)
```